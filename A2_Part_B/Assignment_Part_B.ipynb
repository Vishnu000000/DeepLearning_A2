{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4e69a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b72b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"inaturalist_12K\"\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(f\"{DATA_DIR}/train\", train_transform)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(full_dataset)), \n",
    "    test_size=0.2, \n",
    "    stratify=full_dataset.targets\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    Subset(full_dataset, train_idx),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    Subset(full_dataset, val_idx),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f511d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_resnet_model():\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Modify classifier\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 10)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "model = create_resnet_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a7354",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"1b5f670bdb4b8ed39a9bc34744dd738c9b33dede\") # WandB API Key\n",
    "wandb.init(project=\"DL-Assignment2-ResNet\", config={\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"architecture\": \"ResNet50\"\n",
    "})\n",
    "\n",
    "#1b5f670bdb4b8ed39a9bc34744dd738c9b33dede\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    # Log metrics\n",
    "    val_acc = 100 * correct / len(val_loader.dataset)\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"val_loss\": val_loss/len(val_loader),\n",
    "        \"val_acc\": val_acc\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f230bb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    samples = {i: [] for i in range(10)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for img, label, pred in zip(images, labels, preds):\n",
    "                if len(samples[label.item()]) < 3:\n",
    "                    samples[label.item()].append((\n",
    "                        img.cpu(), \n",
    "                        pred.item()\n",
    "                    ))\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(12, 30))\n",
    "    for cls_idx, (cls_name, examples) in enumerate(samples.items()):\n",
    "        for ex_idx, (img, pred) in enumerate(examples):\n",
    "            ax = axes[cls_idx, ex_idx]\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"True: {class_names[cls_idx]}\\nPred: {class_names[pred]}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"predictions\": plt})\n",
    "\n",
    "# Class names\n",
    "class_names = [\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \n",
    "              \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n",
    "\n",
    "# Generate and log predictions\n",
    "plot_predictions(model, val_loader, class_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
