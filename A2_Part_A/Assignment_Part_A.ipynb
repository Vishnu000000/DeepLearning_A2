{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31240b1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set computation device and default dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32\n",
    "print(f\"Using device: {device}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43725811",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\",\n",
    "    \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"\n",
    "]\n",
    "DATA_URL = \"https://storage.googleapis.com/wandb_dataset/nature_12K.zip\"\n",
    "ZIP_FILE = \"nature_12K.zip\"\n",
    "DATA_FOLDER = \"nature_12K\"\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc78c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_and_extract(url: str, zip_name: str, extract_to: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a zip file from `url` and extract it to `extract_to` directory.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(extract_to):\n",
    "        if not os.path.isfile(zip_name):\n",
    "            os.system(f\"wget {url} -O {zip_name}\")\n",
    "        os.system(f\"unzip -q {zip_name} -d {extract_to}\")\n",
    "        os.remove(zip_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d911f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(\n",
    "    data_dir: str,\n",
    "    batch_size: int,\n",
    "    val_split: float = 0.2,\n",
    "    test_split: float = 0.1,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_val_idx, test_idx = train_test_split(indices, test_size=test_split, random_state=seed)\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_val_idx,\n",
    "        test_size=val_split / (1 - test_split),\n",
    "        random_state=seed\n",
    "    )\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    val_set = Subset(dataset, val_idx)\n",
    "    test_set = Subset(dataset, test_idx)\n",
    "    return (\n",
    "        DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_set, batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168489ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_channels: list,\n",
    "        conv_kernels: list,\n",
    "        hidden_dim: int,\n",
    "        dropout_rate: float = 0.5\n",
    "    ):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for out_ch, kernel in zip(conv_channels, conv_kernels):\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels, out_ch, kernel_size=kernel),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            ]\n",
    "            in_channels = out_ch\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # compute flattened features size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, *IMAGE_SIZE)\n",
    "            feat = self.features(dummy)\n",
    "            flat_size = feat.numel()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_size, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, len(CLASS_NAMES))\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55125022",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform one epoch of training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "    for images, labels in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds += outputs.argmax(1).cpu().tolist()\n",
    "        targets += labels.cpu().tolist()\n",
    "    train_acc = accuracy_score(targets, preds)\n",
    "    return total_loss / len(dataloader), train_acc\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validate\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            preds += outputs.argmax(1).cpu().tolist()\n",
    "            targets += labels.cpu().tolist()\n",
    "    val_acc = accuracy_score(targets, preds)\n",
    "    return total_loss / len(dataloader), val_acc\n",
    "\n",
    "def test_performance(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate classification report and confusion matrix on test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds += outputs.argmax(1).cpu().tolist()\n",
    "            targets += labels.tolist()\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(targets, preds, target_names=CLASS_NAMES))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(targets, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79356fe0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init(\n",
    "        project=\"Deep_Learning_Assignment2_cs24m022\",\n",
    "        config={\n",
    "            \"batch_size\": 32,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"conv_channels\": [16, 32, 64],\n",
    "            \"conv_kernels\": [3, 3, 3],\n",
    "            \"hidden_dim\": 256,\n",
    "            \"dropout_rate\": 0.5,\n",
    "            \"epochs\": 10\n",
    "        }\n",
    "    )\n",
    "    cfg = wandb.config\n",
    "    fetch_and_extract(DATA_URL, ZIP_FILE, DATA_FOLDER)\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        os.path.join(DATA_FOLDER, \"train\"), cfg.batch_size\n",
    "    )\n",
    "    model = SimpleCNN(\n",
    "        cfg.conv_channels,\n",
    "        cfg.conv_kernels,\n",
    "        cfg.hidden_dim,\n",
    "        cfg.dropout_rate\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc\n",
    "        })\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{cfg.epochs}: \"\n",
    "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\"\n",
    "        )\n",
    "    test_performance(model, test_loader)\n",
    "    print(\"Training and evaluation completed.\")\n",
    "\n",
    "# === Cell 8: Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
